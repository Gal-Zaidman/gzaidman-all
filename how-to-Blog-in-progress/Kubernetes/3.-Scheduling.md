# Scheduling

## The kubernetes scheduler

The scheduler is a kubernetes component which is found on the control plain and responsible for deciding on which node a given pod will run.
It is important to note that the scheduler doesn't talk to the container engine and start it, that is the kubelet job. Instead the scheduler talks to the kubeapi-server which talks the the kubelet on the selected node.

### How does the scheduler schedule a pod

If you will inspect a running pod definition yaml you will see a field called nodeName under the spec section. When we create the pod we normally don't set that field, it is added for is when the pod is scheduled by the scheduler.
The scheduler scans the pods periodically and checks if there is a pod with no nodeName on it, if it finds one, the scheduler looks the best node to run the pod. The scheduler has a scheduling algorithm that ranks the nodes from best fit to worse, there are a number of kubernetes schedulers available with different algorithms. Once a node is found the scheduler sets the nodeName field to the selected node.
If the scheduler didn't find a node to run the pod it will remain in Pending state until a node will be available.

### Manual Scheduling

If there is no scheduler or we want to manually select the node that will run a pod, we need to set nodeName in the pod definition yaml.
Notice that we can only set nodeName when the pod is created, Kubernetes will not allow us to modify it if the pod is running.

To schedule a pod that is already defined we need to create a **binding object** and send a post request to the pod binding API.

A binding object looks like:

```yaml
apiVersion: v1
kind: Binding
metadata:
    name: ngix
target:
    apiVersion: v1
    kind: Node
    name: node02
```

Then we need to send a post request with the binding object as json:

```bash
curl –header "Content-Type:application/json" --request POST –data '{"apiVersion":"v1", "kind": "Binding" …}' http://$SERVER/api/v1/namespaces/default/pods/$PODNAME/binding
```

## Labels, Selectors and Annotations

When we use kubernetes after a short time we will end up with a lot of different objects, for example for a simple app we will have a frontend, backend and DB pods, each will probably have its own service, and all of them under a specific deployment, you we will need to have multiple envs such a production and development and in a short time you will have hundreds of components in you cluster.
To be able to control and filter objects we will assign labels to them.
Label are just key-value items assigned to kubernetes objects in the metadata section on the object definition file.
Selector enables us to select specific objects by their labels.
You can think of labels as like tags in a online store (color red, condition new) and the selectors as the filters.

To add a label to an existing object we use the label command:

```bash
kubectl label OBJECT OBJECT_NAME LABEL=VALUE
#for example
kubectl label node node1 size=Large
```

To select a object by a label we use the --selector

```bash
kubectl get <object> --selector app=some-app
```

But there is more to labels and selectors then just an easy way to organize your objects.
Kubernetes uses labels and selectors internally to connect objects to each other. For example when we have a deployment it contains a replica set which will contain some pods it manages, to connect the those components the containing object will specify the labels of the contained object in the selector field:

```yaml
# Deployment
$ kubectl get deployments.apps  -o yaml
apiVersion: v1
items:
- apiVersion: apps/v1
  kind: Deployment
  metadata:
...
# Deployment labels
    labels:
      run: http
  spec:
...
# select replica sets to manage
    selector:
      matchLabels:
        run: http
...

$ kubectl get replicasets.apps  -o yaml
apiVersion: v1
items:
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
...
# ReplicaSet labels
    labels:
      pod-template-hash: 774bb756bb
      run: http
...
  spec:
# select pods to manage
    selector:
      matchLabels:
        pod-template-hash: 774bb756bb
        run: http
    template:
      metadata:
        creationTimestamp: null
# Created pod labels
        labels:
          pod-template-hash: 774bb756bb
          run: http

$ kubectl get pods  -o yaml
apiVersion: v1
items:
- apiVersion: v1
  kind: Pod
  metadata:
...
# Pod labels
    labels:
      pod-template-hash: 774bb756bb
      run: http
...
```

What if we just want to add some information to a object, for example the version but don't want to make this information filterable?
That is where annotations come into play, under the metadata section we can add annotations to an object to specify information on that pod.

https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/

## Taints and Tolerations:

Taints and tolerations lets us restrict which pods can be scheduled on which nodes.
Taint are used for Nodes and Tolerations are for pods. When we add a taint to a node only pods that can tolerate that specific taint can be scheduled on it. By default a new pod has no tolerations meaning that the pod can't tolerate any taints unless we specify it.

On each taint we add we need to specify the taint effect. Taint effect describe what would happen to the pods that doesn't tolerate this taint, and there are 3 types:

- Noschedule: which is the one we talked about, new pods will not be scheduled on that node.
- PreferNoschedule: That means that the node will be ranked the lowest of the available nodes for a pod that doesn't tolerate that taint.
- Noexecute: new pods will not be scheduled on that node and if there is a pod on the node that doesn't tolerant the taint it will be evicted.

To add a taint to a node we run:

```bash
kubectl taint nodes <node-name> key=value:taint-effect
```

To add a toleration we need to edit the pod yaml and add a toleration section:

spec:

```yaml
tolerations:
- key: "app"
  operator: "Equal"
  value: "blue"
  effect: "Noschedule"
```

Notice that all the values needs to be surrounded in "".

## NodeSelectors and NodeAffinity

Taints and Tolerations allowed us to restrict which pods can be scheduled on which nodes, but what if we want to specify specific nodes where we want the pod to run ?
well we can force it add and taint all the nodes and pods beside one but that will makes managing the cluster very complex!

Thats where NodeSelectors and NodeAffinity come into play.
Node selectors are basically regular selectors which we can add to a pod definition to select the node size it run. NodeSelectors match a specific label "size" which we need to add to the node.
For example in our env we have 3 nodes, and node2 has the most resources, we want to create a pod(pod1) that will need a lot of resources so we want to to always run on big nodes such has node2.
We first need to add label large to node2:

```bash
kubectl label node node2 size=Large
```

Then when we create pod1 we add the nodeSelector field to the spec section:

```yaml
...
spec:
  nodeSelector:
    size: "Large"
  ...
```

Node selectors are great for the simple case, but sometimes we want to use more complex expressions to select nodes.
Node affinity is the complex form of node selector, it enables us to select multiple values and to use more complex expressions. Node affinity has 3 types, which state when do we care about the affinity:

- RequiredDuringSchedulingIgnoredDuringExecution.
- PrefferedDuringSchedulingIgnoredDuringExecution.
- RequiredDuringSchedulingRequiredDuringExecution (Planned)

Node affinity is much more complex then Node selector:

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: with-node-affinity
spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: kubernetes.io/e2e-az-name
            operator: In
            values:
            - e2e-az1
            - e2e-az2
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 1
        preference:
          matchExpressions:
          - key: another-node-label-key
            operator: In
            values:
            - another-node-label-value
  containers:
  - name: with-node-affinity
```

Some notes:

- Valid operators are: In, NotIn, Exists, DoesNotExist, Gt, Lt.
- Multiple nodeSelectorTerms has a OR between them, meaning the pod will scheduled onto a node only if **any** nodeSelectorTerms is satisfied.
- Multiple matchExpressions have a AND between them, meaning the pod will scheduled onto a node only if **all** matchExpressions is satisfied.
- The weight field is in the range 1-100. For each node that meets all of the scheduling requirements, the scheduler will compute a sum by iterating through the elements of this field and adding "weight" to the sum if the node matches the corresponding MatchExpressions. This score is then combined with the scores of other priority functions for the node. The node with the highest total score are the most preferred.

## Resource requirements and limits:

We can define resources requirements and resource usage limits for each container in the pod.
When the pod will be scheduled then Kubernetes will look for a node which meets all the requirements, if none is found then the pod will not be scheduled and remain in Pending state.
When we specify limits, if a container exceeds its limits constantly then it will be removed.

We can specify the following resources:

- CPU: the time/amount of CPU we want to dedicate for the container. One cpu, in Kubernetes, is equivalent to 1 vCPU/Core for cloud providers and 1 hyperthread on bare-metal Intel processors.
  Possible values are:
  - floats and ints: from 0.1
  - millicpu: from 1m (0.1 == 100)
  A continer cannot exceed the CPU limit, kubernetes will throttle the CPU of the container

- Memory: measured in bytes. You can express memory as a plain integer or as a fixed-point integer using one of these suffixes: G, M, K. You can also use the power-of-two equivalents: Gi, Mi, Ki.
A container can exceed its mem limit for a short time, but if that occurs more frequently then the pod will be removed.

## DaemonSets:

A DaemonSet ensures that all (or some) Nodes run a copy of a Pod. As nodes are added to the cluster, Pods are added to them. As nodes are removed from the cluster, those Pods are garbage collected. Deleting a DaemonSet will clean up the Pods it created.

Some typical uses of a DaemonSet are:

- running a cluster storage daemon on every node
- running a logs collection daemon on every node
- running a node monitoring daemon on every node

DaemonSet pods are created and scheduled by the DaemonSet controller and not by the Kubernetes scheduler.

### Defining a DaemonSet

Required Fields, As with all other Kubernetes config, a DaemonSet needs apiVersion, kind, and metadata fields, and a spec section which contains a selector and template

Here is an example od a daemon set definition file:

```yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: fluentd-elasticsearch
  namespace: kube-system
  labels:
    k8s-app: fluentd-logging
spec:
  selector:
    matchLabels:
      name: fluentd-elasticsearch
  template:
    metadata:
      labels:
        name: fluentd-elasticsearch
    spec:
      tolerations:
      # this toleration is to have the daemonset runnable on master nodes
      # remove it if your masters can't run pods
      - key: node-role.kubernetes.io/master
        effect: NoSchedule
      containers:
      - name: fluentd-elasticsearch
        image: quay.io/fluentd_elasticsearch/fluentd:v2.5.2
        resources:
          limits:
            memory: 200Mi
          requests:
            cpu: 100m
            memory: 200Mi
        volumeMounts:
        - name: varlog
          mountPath: /var/log
        - name: varlibdockercontainers
          mountPath: /var/lib/docker/containers
          readOnly: true
      terminationGracePeriodSeconds: 30
      volumes:
      - name: varlog
        hostPath:
          path: /var/log
      - name: varlibdockercontainers
        hostPath:
          path: /var/lib/docker/containers
```

## Static Pods

Static pods are pods which are managed internally in the Node by kubelet and not by the kube-api like most pods. Kubelet scans the directory which is defined in its configuration file by the staticPodPath field the default location is /etc/kubernetes/manifests. Kubelet monitors that directory so if we modify one of the files there then kubelet will apply the changes to the pod, those pods are known as static pods.

To set that dir we can:

Edit the kubelet.service file –pod-manifest-path option.

Edit the kubelet.service file and add an option –config there which will point to a yaml file, which will include: staticPodPath: <path to dir>

## Multi Scheduallers

In a kubernetics env we can have multipule schedulers, this is useful when we need a specific scheduling algorithm.

By default when we start kubernetics create the default scheduler, we can recognize it because its name will be default-scheduller.

To create a scheduler the easest way is to go to /etc/kubernetes/manifests and copy the scheduler yaml, on it we need to change the name, and add to the command a –scheduler-name arg. After we set the scheduler we can create a pod and in its spec section we need to add a schedulerName, and set leader-elect to false. 
